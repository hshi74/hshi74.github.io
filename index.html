<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Haochen Shi</title>
  <meta name="author" content="Haochen Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="static/favicon.png">
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KVK6EML3WQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-KVK6EML3WQ');
  </script>
</head>

<body>
  <section class="introduction">
    <div class="bio">
      <h2 class="name">Haochen Shi</h2>
      <p>
        I am a second-year CS Ph.D. student at Stanford, advised by <a
          href="https://tml.stanford.edu/people/karen-liu">C. Karen Liu</a> and <a
          href="https://shurans.github.io/">Shuran
          Song</a>. During my Master's at Stanford, my research
        focused on robotics and computer vision, more specifically deformable object manipulation, in
        collaboration with <a href="https://jiajunwu.com/">Jiajun Wu</a>, <a href="http://hxu.rocks/">Huazhe Xu</a>, and
        <a href="https://yunzhuli.github.io/">Yunzhu Li</a>. Previously, during undergraduate at UW-Madison, my work
        with <a href="https://gleicher.sites.cs.wisc.edu/">Michael Gleicher</a> and <a
          href="https://dannyrakita.net/">Danny Rakita</a> explored motion planning algorithms for robots.
      </p>
      <p>
        I'm interested in challenging robotic manipulation and locomotion tasks.
        In my leisure time, I love playing tennis, Go, chess, and mahjong.
      </p>
      <p style="text-align:center">
        <a href="mailto:hshi74@stanford.edu">Email</a> &nbsp/&nbsp
        <a href="https://scholar.google.com/citations?hl=en&user=gr51Y7UAAAAJ">Google Scholar</a>
        &nbsp/&nbsp
        <a href="https://x.com/HaochenShi74">X</a> &nbsp/&nbsp
        <a href="https://github.com/hshi74">Github</a>
      </p>
    </div>
    <img class="profile-photo" src="static/me.jpeg" alt="Haochen Shi">
  </section>

  <section class="research">
    <h2>Research</h2>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/rtr.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids</b>
        <p class="paper-authors">
          <a href="https://hukz18.github.io/">Kaizhe Hu</a><sup>*</sup>,
          <b>Haochen Shi</b><sup>*</sup>,
          <a href="https://shockwavehe.github.io/">Yao He</a>,
          <a href="http://me.weizhuowang.com/">Weizhuo Wang</a>,
          <a href="https://profiles.stanford.edu/c-karen-liu">C. Karen Liu</a><sup>†</sup>,
          <a href="https://shurans.github.io/">Shuran Song</a><sup>†</sup>
        </p>
        <p class="paper-details"><sup>*</sup>Equal contribution, <sup>†</sup>Equal advising</p>
        <p class="paper-details"><em>CoRL</em>, 2025</p>
        <div class="paper-links">
          <a href="https://robot-trains-robot.github.io/">[Project Page]</a>
          <a href="https://arxiv.org/abs/2508.12252">[Paper]</a>
          <a href="https://youtu.be/MhwVx_mlm6Q?si=88tZ37hfoI1xbbOz">[Video]</a>
          <a href="https://github.com/hukz18/Robot-Trains-Robot">[Code]</a>
          <a href="https://x.com/hkz222/status/1957632595220660442">[Tweet]</a>
        </div>
        <p class="paper-description">Simulation-based reinforcement learning (RL) has significantly advanced humanoid
          locomotion tasks, yet direct real-world RL from scratch or starting from pretrained policies remains rare,
          limiting the full potential of humanoid robots. Real-world training, despite being crucial for overcoming the
          sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To
          address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher
          actively supports and guides a humanoid student robot.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/toddlerbot.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation</b>
        <p class="paper-authors">
          <b>Haochen Shi</b><sup>*</sup>,
          <a href="http://me.weizhuowang.com/">Weizhuo Wang</a><sup>*</sup>,
          <a href="https://shurans.github.io/">Shuran Song</a><sup>†</sup>,
          <a href="https://profiles.stanford.edu/c-karen-liu">C. Karen Liu</a><sup>†</sup>
        </p>
        <p class="paper-details"><sup>*</sup>Equal contribution, <sup>†</sup>Equal advising</p>
        <p class="paper-details"><em>CoRL</em>, 2025</p>
        <div class="paper-links">
          <a href="https://toddlerbot.github.io/">[Project Page]</a>
          <a href="https://arxiv.org/abs/2502.00893">[Paper]</a>
          <a href="https://youtu.be/A43QxHSgLyM">[Video]</a>
          <a href="https://github.com/hshi74/toddlerbot">[Code]</a>
          <a href="https://x.com/HaochenShi74/status/1886599720279400732">[Tweet]</a>
          <a href="https://hshi74.github.io/toddlerbot">[Docs]</a>
        </div>
        <p class="paper-description">Learning-based robotics research driven by data demands a new approach to robot
          hardware design-one that serves as both a platform for policy execution and a tool for embodied data
          collection to train policies. We introduce ToddlerBot, a low-cost, open-source humanoid robot platform
          designed for scalable policy learning and research in robotics and AI.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/piano.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance</b>
        <p class="paper-authors">
          <a href="https://cs.stanford.edu/~rcwang/">Ruocheng Wang</a><sup>*</sup>,
          <a href="https://pei-xu.github.io/">Pei Xu</a><sup>*</sup>,
          <b>Haochen Shi</b>,
          <a href="https://music.stanford.edu/people/elizabeth-schumann">Elizabeth Schumann</a>,
          <a href="https://profiles.stanford.edu/c-karen-liu">C. Karen Liu</a>
        </p>
        <p class="paper-details"><sup>*</sup>Equal contribution</p>
        <p class="paper-details"><em>SIGGRAPH Asia</em>, 2024</p>
        <div class="paper-links">
          <a href="https://for-elise.github.io/">[Project Page]</a>
          <a href="https://arxiv.org/pdf/2410.05791">[Paper]</a>
          <!-- <a href="https://github.com/hshi74/piano_dexterity">[Code]</a> -->
        </div>
        <p class="paper-description">Piano playing requires agile, precise, and coordinated hand control that
          stretches the limits of dexterity. Hand motion models with the sophistication
          to accurately recreate piano playing have a wide range of applications in
          character animation, embodied AI, biomechanics, and VR/AR. In this paper,
          we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion
          and audio from 15 elite-level pianists playing 153 pieces of classical music.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/dexcap.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</b>
        <p class="paper-authors">
          <a href="https://www.chenwangjeremy.net/">Chen Wang</a>,
          <b>Haochen Shi</b>,
          <a href="http://me.weizhuowang.com/">Weizhuo Wang</a>,
          <a href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a>,
          <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>,
          <a href="https://profiles.stanford.edu/c-karen-liu">C. Karen Liu</a>
        </p>
        <p class="paper-details"><em>RSS</em>, 2024</p>
        <div class="paper-links">
          <a href="https://dex-cap.github.io/">[Project Page]</a>
          <a href="https://arxiv.org/abs/2403.07788">[Paper]</a>
          <a href="https://youtu.be/-PmjRjgXKuo">[Video]</a>
          <a href="https://github.com/j96w/DexCap">[Code]</a>
        </div>
        <p class="paper-description">Imitation learning from human hand motion data presents a promising avenue for
          imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial
          challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the
          difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce
          DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training
          dexterous robot skills directly from human hand mocap data.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/robopack.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing</b>
        <p class="paper-authors">
          <a href="https://albertboai.com/">Bo Ai</a><sup>*</sup>,
          <a href="https://s-tian.github.io/">Stephan Tian</a><sup>*</sup>,
          <b>Haochen Shi</b>,
          <a href="https://wangyixuan12.github.io/">Yixuan Wang</a>,
          <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan">Cheston Tan</a>,
          <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
          <a href="https://jiajunwu.com/">Jiajun Wu</a>
        </p>
        <p class="paper-details"><sup>*</sup>Equal contribution</p>
        <p class="paper-details"><em>RSS</em>, 2024</p>
        <p class="paper-details">Abridged in ICRA 2024 workshops: ViTac, 3DVRM, and Future Roadmap for Manipulation
          Skills</p>
        <div class="paper-links">
          <a href="https://robo-pack.github.io/">[Project Page]</a>
          <a href="https://arxiv.org/abs/2407.01418">[Paper]</a>
        </div>
        <p class="paper-description">Tactile feedback is critical for understanding the dynamics of both rigid and
          deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We
          introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural,
          tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to
          estimate object states, including particles and object-level latent physics information, from historical
          visuo-tactile observations and to perform future state predictions.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/robocraft3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">RoboCraft: Learning to see, simulate, and shape elasto-plastic objects in 3D with graph
          networks</b>
        <p class="paper-authors">
          <b>Haochen Shi</b><sup>*</sup>,
          <a href="http://hxu.rocks/">Huazhe Xu</a><sup>*</sup>,
          <a href="https://sites.google.com/view/zhiao-huang/">Zhiao Huang</a>,
          <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
          <a href="https://jiajunwu.com/">Jiajun Wu</a>
        </p>
        <p class="paper-details"><sup>*</sup>Equal contribution</p>
        <p class="paper-details"><em>The International Journal of Robotics Research (IJRR)</em></p>
        <div class="paper-links">
          <a href="http://hshi74.github.io/robocraft3d">[Project Page]</a>
          <a href="https://doi.org/10.1177/02783649231219020">[Paper]</a>
          <a href="https://github.com/hshi74/robocraft3d">[Code]</a>
        </div>
        <p class="paper-description">Modeling and manipulating elasto-plastic objects are essential capabilities for
          robots to perform complex industrial and household interaction tasks (e.g., stuffing dumplings, rolling sushi,
          and making pottery). However, due to the high degrees of freedom of elasto-plastic objects, significant
          challenges exist in virtually every aspect of the robotic manipulation pipeline, e.g., representing the
          states, modeling the dynamics, and synthesizing the control signals. We propose to tackle these challenges by
          employing a particle-based representation for elasto-plastic objects in a model-based planning framework.
        </p>
    </article>

    <!-- <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/rtx.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</b>
        <p class="paper-authors">
          <a href="https://robotics-transformer-x.github.io/">Open X-Embodiment Collaboration</a>
        </p>
        <p class="paper-details"><em>ICRA</em>, 2024</p>
        <div class="paper-links">
          <a href="https://robotics-transformer-x.github.io">[Project Page]</a>
          <a href="https://robotics-transformer-x.github.io/paper.pdf">[Paper]</a>
        </div>
        <p class="paper-description">Large, high-capacity models trained on diverse datasets have shown remarkable
          successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision,
          this has led to a consolidation of pretrained models, with general pretrained backbones
          serving as a starting point for many applications.
          Can such a consolidation happen in robotics?
        </p>
    </article> -->

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/robocook.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</b>
        <p class="paper-authors">
          <b>Haochen Shi</b><sup>*</sup>,
          <a href="http://hxu.rocks/">Huazhe Xu</a><sup>*</sup>,
          <a href="https://samuelpclarke.com/">Samuel Clarke</a>,
          <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
          <a href="https://jiajunwu.com/">Jiajun Wu</a>
        </p>
        <p class="paper-details"><sup>*</sup>Equal contribution</p>
        <p class="paper-details"><em>CoRL</em>, 2023</p>
        <b class="paper-award">Best System Paper</b>
        <div class="paper-links">
          <a href="https://hshi74.github.io/robocook">[Project Page]</a>
          <a href="https://arxiv.org/abs/2306.14447">[Paper]</a>
          <a href="https://github.com/hshi74/robocook">[Code]</a>
        </div>
        <p class="paper-description">Humans excel in complex long-horizon soft body manipulation tasks via flexible tool
          use: bread baking requires a knife to slice the dough and a rolling pin to flatten it. Often regarded as a
          hallmark of human cognition, tool use in autonomous robots remains limited due to challenges in
          understanding tool-object interactions. Here we develop an intelligent robotic system, RoboCook,
          which perceives, models, and manipulates elasto-plastic objects with various tools.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/robocraft.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph
          Networks</b>
        <p class="paper-authors">
          <b>Haochen Shi</b><sup>*</sup>,
          <a href="http://hxu.rocks/">Huazhe Xu</a><sup>*</sup>,,
          <a href="https://sites.google.com/view/zhiao-huang/">Zhiao Huang</a>,
          <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
          <a href="https://jiajunwu.com/">Jiajun Wu</a>
        </p>
        <p class="paper-details"><sup>*</sup>Equal contribution</p>
        <p class="paper-details"><em>RSS</em>, 2022</p>
        <p class="paper-details">Abridged in ICRA 2022 workshop on Representing and Manipulating Deformable Objects</p>
        <div class="paper-links">
          <a href="http://hxu.rocks/robocraft">[Project Page]</a>
          <a href="https://arxiv.org/abs/2205.02909">[Paper]</a>
          <a href="https://github.com/hshi74/RoboCraft">[Code]</a>
        </div>
        <p class="paper-details">
          Covered by
          <a href="https://news.mit.edu/2022/robots-play-play-dough-0623">[MIT News]</a>
          <a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh">[Stanford HAI]</a>
          <a
            href="https://www.newscientist.com/article/2325970-ai-powered-robot-learned-to-make-letters-out-of-play-doh-on-its-own/">[New
            Scientist]</a>
        </p>
        <p class="paper-description">Modeling and manipulating elasto-plastic objects are essential capabilities
          for robots to perform complex industrial and household interaction tasks. However, significant
          challenges exist in virtually every aspect of the robotic manipulation pipeline,
          e.g., representing the states, modeling the dynamics, and synthesizing
          the control signals. We propose to tackle these challenges by employing a
          particle-based representation for elasto-plastic objects in a model-based planning framework.
        </p>
    </article>

    <article class="paper">
      <div class="paper-media">
        <video class="paper-video" width="100%" height="100%" muted autoplay loop>
          <source src="static/cik.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="paper-content">
        <b class="paper-title">CollisionIK: A per-instant pose optimization method for generating robot motions with
          environment collision avoidance</b>
        <p class="paper-authors">
          <a href="https://pages.cs.wisc.edu/~rakita/">Daniel Rakita</a>,
          <b>Haochen Shi</b>,
          <a href="http://bilgemutlu.com/">Bilge Mutlu</a>,
          <a href="https://gleicher.sites.cs.wisc.edu/">Michael Gleicher</a>
        </p>
        <p class="paper-details"><em>ICRA</em>, 2021</p>
        <div class="paper-links">
          <a href="https://arxiv.org/abs/2102.13187">[Paper]</a>
          <a href="https://github.com/uwgraphics/relaxed_ik_core">[Code]</a>
          <a href="https://youtu.be/rdMl1gOPNoM">[Talk]</a>
        </div>
        <p class="paper-description">In this work, we present a per-instant pose optimization method that can generate
          configurations that achieve specified pose or motion objectives as best as possible over a sequence of
          solutions, while also simultaneously avoiding collisions with static or dynamic obstacles in the environment.
        </p>
    </article>
  </section>

  <section class="teaching">
    <h2>Teaching</h2>
    <ul>
      <li>Teaching Assistant, <a href="http://cs231n.stanford.edu/">Stanford CS 231N</a>, Spring 2022, Spring 2023</li>
      <li>Teaching Assistant, <a href="https://web.stanford.edu/class/cs109/">Stanford CS 109</a>, Fall 2021, Winter
        2022 </li>
      <li>Peer Mentor, <a href="https://graphics.cs.wisc.edu/Courses/559-sp2020/">UW-Madison CS 559</a>,
        Spring 2020</li>
    </ul>
  </section>

  <section class="talks">
    <h2>Talks</h2>
    <ul>
      <li>[2024/01/10] Invited Talk at <a href="https://www.techbeat.net/">TechBeat</a> <a
          href="https://www.bilibili.com/video/BV1LQ4y157ib/?share_source=copy_web&vd_source=ebf557da212f50c089d878c01d9c7af0">[Recording
          in Chinese]</a>
      </li>
      <li>[2024/01/06] Invited Talk at <a href="https://caai.cn/">CAAI</a> <a
          href="https://www.bilibili.com/video/BV1gC4y1v7wS/?share_source=copy_web&vd_source=ebf557da212f50c089d878c01d9c7af0">[Recording
          in Chinese]</a>
      </li>
      <li>[2023/05/28] Invited Talk at Tsinghua University</li>
    </ul>
  </section>

  <section class="service">
    <h2>Professional Service</h2>
    <ul>
      <li>Conference Reviewer: IROS (2023, 2024, 2025), CoRL (2023, 2024, 2025), ICRA (2025), ICLR (2025), RSS (2025),
        Humanoids (2025)</li>
      <li>Journal Reviewer: RA-L</li>
    </ul>
  </section>

  <footer>
    <p>Template from <a href="https://jonbarron.info/">Jon Barron's website</a></p>
  </footer>

</body>

</html>